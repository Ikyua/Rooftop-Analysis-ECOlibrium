{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My comments may seem a bit verbose because the intention is for the next intern to understand where I left off. Of course do your own research as I could be wring and there are probably better ways of doing this that I don't know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load the libraries\n",
    "# You may need to use conda in instead of pip to install torch and by extension the rest of these. A second \n",
    "# A second issue that you might have is trying to do any sort of deep learning like what you're doing here\n",
    "# without a GPU, namely one that has CUDA. You can still do it this way using your CPU but it'll run REALLY slow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import timm\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to use geemap (gee stands for Google Earth Engine) to get satellite images of the roofs. But to be honest, if you can find a good dataset with ALOT of rooftop images, use that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geemap\n",
    "import ee\n",
    "\n",
    "# Initialize the Earth Engine API\n",
    "ee.Authenticate()\n",
    "ee.Initialize(project= \"ee-loisaidaalbedo24\")\n",
    "\n",
    "\n",
    "# Create a map\n",
    "Map = geemap.Map()\n",
    "\n",
    "# Example: Define an area of interest by coordinates\n",
    "aoi = ee.Geometry.Polygon([[\n",
    "[40.7661, -73.9876],\n",
    "[40.717087, -73.984536],\n",
    "[40.713645, -73.988087],\n",
    "[40.710536, -73.991699],\n",
    "[40.708964, -73.991834],\n",
    "[40.706721, -73.987287]\n",
    "  ]])\n",
    "\n",
    "# Create a map centered on the AOI\n",
    "Map = geemap.Map(center=[40.748, -73.985], zoom=12)\n",
    "Map.addLayer(image, {'bands': ['B4', 'B3', 'B2'], 'max': 3000}, 'Satellite Image')\n",
    "Map.addLayer(aoi, {}, 'AOI')\n",
    "\n",
    "# Display the map\n",
    "Map\n",
    "\n",
    "\n",
    "# Get a satellite image collection\n",
    "collection = ee.ImageCollection('COPERNICUS/S2') \\\n",
    "    .filterDate('2023-01-01', '2023-12-31') \\\n",
    "    .filterBounds(aoi) \\\n",
    "    .sort('CLOUD_COVER') \\\n",
    "    .limit(10)\n",
    "\n",
    "# Select the image band for visualization, e.g., True Color\n",
    "image = collection.median().clip(aoi)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Map = geemap.Map(center=[40.748, -73.985], zoom=12)\n",
    "Map.addLayer(image, {'bands': ['B4', 'B3', 'B2'], 'max': 3000}, 'Satellite Image')\n",
    "Map.addLayer(aoi, {}, 'AOI')\n",
    "\n",
    "Map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RooftopDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform = None):\n",
    "        self.data = ImageFolder(data_dir, transform=transform)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]\n",
    "    def classes(self):\n",
    "        return self.data.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "SyntaxError",
     "evalue": "expected argument value expression (2725475908.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    dataset = RooftopDataset(data_dir = )\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected argument value expression\n"
     ]
    }
   ],
   "source": [
    "dataset = RooftopDataset(data_dir = )\n",
    "\n",
    "#Use Kyle's code to get an approximate albedo value for the labels. This is a crude estimator but, it's better than nothing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a dictionary that matches the images to their classifications\n",
    "data_dir = #Dataset               \n",
    "\n",
    "pic_to_class = {v: k for k, v in ImageFolder(data_dir).class_to_idx.items()}\n",
    "print(pic_to_class)\n",
    "\n",
    "#Normalize and make the images into tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the CNN, look it up so you know how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlbedoEstimator(nn.Module):\n",
    "    def __init__(self, num_colors = 2):\n",
    "        super(AlbedoEstimator,self).__init__()\n",
    "        self.base_model = timm.create_model('efficientnet_b0',pretrained=True)\n",
    "        self.features = nn.Sequential(*list(self.base_model.children())[:-1])\n",
    "        \n",
    "        enet_out_size = 1280\n",
    "\n",
    "        self.estimator = nn.Linear(enet_out_size,num_colors)\n",
    "    \n",
    "    def forward(self, x):\n",
    "       # Pass input through the feature extractor\n",
    "        x = self.features(x)\n",
    "        # Flatten the output of the feature extractor\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass the flattened output through the linear layer\n",
    "        x = self.estimator(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlbedoEstimator(num_colors = 2)\n",
    "print(model)\n",
    "\n",
    "#Use softmax for the prob. estimation layer:\n",
    "softmax = nn.Softmax(dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = #insert file path here\n",
    "\n",
    "test_folder = #insert file path here\n",
    "\n",
    "val_folder = #insert file path here\n",
    "\n",
    "train_data = RooftopDataset(train_folder,transform = transform)\n",
    "\n",
    "test_data = RooftopDataset(test_folder,transform = transform)\n",
    "\n",
    "val_data = RooftopDataset(val_folder,transform = transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "train_losses , val_losses = [],[]\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "model = AlbedoEstimator(num_colors=2)\n",
    "model.to(device)\n",
    "\n",
    "#Establish loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#Get an optimizer, for now I can just use ADAM \n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = AlbedoEstimator(num_colors=2)\n",
    "model.to(device)\n",
    "\n",
    "# Establish loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get an optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc='Training loop'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        classifications = model(images)\n",
    "        loss = loss_function(classifications, labels)  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc='Validation loop'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "         \n",
    "            classifications = model(images)\n",
    "            loss = loss_function(classifications, labels)  # Use raw logits here\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "    val_loss = running_loss / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss}, Validation loss: {val_loss}\")\n",
    "\n",
    "# Get the probabilities for each classification. These will be interpreted as the albedo values\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images.to(device)\n",
    "        classifications = model(images)\n",
    "        albedo = torch.softmax(classifications, dim=1) \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
