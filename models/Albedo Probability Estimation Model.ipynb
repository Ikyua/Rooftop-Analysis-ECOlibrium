{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My comments may seem a bit verbose because the intention is for the next intern to understand where I left off. Of course do your own research as I could be wrong and there are probably better ways of doing this that I don't know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load the libraries\n",
    "# You may need to use conda in instead of pip to install torch and by extension the rest of these. A second \n",
    "# A second issue that you might have is trying to do any sort of deep learning like what you're doing here\n",
    "# without a GPU, namely one that has CUDA. You can still do it this way using your CPU but it'll run REALLY slow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import timm\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to use geemap (gee stands for Google Earth Engine) to get satellite images of the roofs. But to be honest, if you can find a good dataset with ALOT of rooftop images, use that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geemap\n",
    "import ee\n",
    "\n",
    "# Initialize the Earth Engine API\n",
    "ee.Authenticate()\n",
    "ee.Initialize(project= \"ee-loisaidaalbedo24\")\n",
    "\n",
    "\n",
    "# Create a map\n",
    "Map = geemap.Map()\n",
    "\n",
    "# Example: Define an area of interest by coordinates\n",
    "aoi = ee.Geometry.Polygon([[\n",
    "[40.7661, -73.9876],\n",
    "[40.717087, -73.984536],\n",
    "[40.713645, -73.988087],\n",
    "[40.710536, -73.991699],\n",
    "[40.708964, -73.991834],\n",
    "[40.706721, -73.987287]\n",
    "  ]])\n",
    "\n",
    "# Create a map centered on the AOI\n",
    "Map = geemap.Map(center=[40.748, -73.985], zoom=12)\n",
    "Map.addLayer(image, {'bands': ['B4', 'B3', 'B2'], 'max': 3000}, 'Satellite Image')\n",
    "Map.addLayer(aoi, {}, 'AOI')\n",
    "\n",
    "# Display the map\n",
    "Map\n",
    "\n",
    "\n",
    "# Get a satellite image collection\n",
    "collection = ee.ImageCollection('COPERNICUS/S2') \\\n",
    "    .filterDate('2023-01-01', '2023-12-31') \\\n",
    "    .filterBounds(aoi) \\\n",
    "    .sort('CLOUD_COVER') \\\n",
    "    .limit(10)\n",
    "\n",
    "# Select the image band for visualization, e.g., True Color\n",
    "image = collection.median().clip(aoi)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Map = geemap.Map(center=[40.748, -73.985], zoom=12)\n",
    "Map.addLayer(image, {'bands': ['B4', 'B3', 'B2'], 'max': 3000}, 'Satellite Image')\n",
    "Map.addLayer(aoi, {}, 'AOI')\n",
    "\n",
    "Map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RooftopDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform = None):\n",
    "        self.data = ImageFolder(data_dir, transform=transform)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]\n",
    "    def classes(self):\n",
    "        return self.data.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RooftopDataset(data_dir = )\n",
    "\n",
    "#It may be helpful to find some crude way of estimating historical albedo values. Here's Kyle's attempt at it.\n",
    "from PIL import Image\n",
    "\n",
    "def get_average_pixel_brightness(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    pixels = list(image.getdata())\n",
    "\n",
    "    # Ensure pixels are in RGB format\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "        pixels = list(image.getdata())\n",
    "\n",
    "    # Calculate normalized brightness for each pixel\n",
    "    normalized_brightness = [\n",
    "        (0.299 * pixel[0] + 0.587 * pixel[1] + 0.114 * pixel[2]) / 255\n",
    "        for pixel in pixels\n",
    "    ]\n",
    "\n",
    "    # Calculate the average brightness of all pixels\n",
    "    average_brightness = sum(normalized_brightness) / len(normalized_brightness)\n",
    "\n",
    "    return average_brightness\n",
    "\n",
    "image_path = 'rooftop.png'\n",
    "average_brightness = get_average_pixel_brightness(image_path)\n",
    "print(\"Average Brightness:\", average_brightness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Kyle did this to add variety to the images\n",
    "\n",
    "#This file will create different variations of an image given by the user (currently manually inputted).\n",
    "\n",
    "#Input: Image file, png, jpg, etc\n",
    "#Output: Original Image, Grey-scaled image, Edged Image, Contrasted Image\n",
    "# Kyle Silvestre\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def saveImage(file_name, image_to_save, directory) -> None:\n",
    "    os.chdir(directory)\n",
    "    cv2.imwrite(file_name, image_to_save)\n",
    "    return None\n",
    "\n",
    "# Manually choosing the directory to save the created image (End-Game: Choose directory to save images)\n",
    "users_chosen_directory = 'C:/Users/k1m4s/Loisaida/Loisaida_internship/created_images' #change path for different directory\n",
    "\n",
    "# Manually loading the selected image (End-Game: Input an image and would change the imagw)\n",
    "users_original_image = cv2.imread('../original_images/building_with_solar_panels.png') #change path for different images\n",
    "\n",
    "#Displaying original image\n",
    "cv2.imshow('Original Image', users_original_image)\n",
    "\n",
    "\n",
    "gray_image = cv2.cvtColor(users_original_image, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('Gray-scale-image', gray_image)\n",
    "saveImage('GreyImage.png', gray_image, users_chosen_directory)\n",
    "\n",
    "#Edge's of the image\n",
    "edge_image = cv2.Canny(users_original_image, 150, 200)\n",
    "cv2.imshow('Edge-image', edge_image)\n",
    "saveImage('Edge-Image.png', edge_image, users_chosen_directory)\n",
    "\n",
    "#Contrasting an Image & Saving it\n",
    "#Alpha ranges from 1.0 - 3.0 | x > 1 increases contrast, x < 1 decreases contrast\n",
    "#Beta ranges from 0 - 100 | x > 0 increases brightness, x < 0 decreases brightness\n",
    "\n",
    "constrastedImage= cv2.convertScaleAbs(users_original_image, 3.0, 2)\n",
    "cv2.imshow('Contrasted Image', constrastedImage)\n",
    "saveImage('ContrastedImage.png', constrastedImage, users_chosen_directory)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a dictionary that matches the images to their classifications\n",
    "data_dir = #Dataset               \n",
    "\n",
    "pic_to_class = {v: k for k, v in ImageFolder(data_dir).class_to_idx.items()}\n",
    "print(pic_to_class)\n",
    "\n",
    "#Normalize and make the images into tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the CNN, look it up so you know how it works. Of course, tweak the code as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlbedoEstimator(nn.Module):\n",
    "    def __init__(self, num_colors = 2):\n",
    "        super(AlbedoEstimator,self).__init__()\n",
    "        self.base_model = timm.create_model('efficientnet_b0',pretrained=True)\n",
    "        self.features = nn.Sequential(*list(self.base_model.children())[:-1])\n",
    "        \n",
    "        enet_out_size = 1280\n",
    "\n",
    "        self.estimator = nn.Linear(enet_out_size,num_colors)\n",
    "    \n",
    "    def forward(self, x):\n",
    "       # Pass input through the feature extractor\n",
    "        x = self.features(x)\n",
    "        # Flatten the output of the feature extractor\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass the flattened output through the linear layer\n",
    "        x = self.estimator(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlbedoEstimator(num_colors = 2)\n",
    "print(model)\n",
    "\n",
    "#Use softmax for the prob. estimation layer:\n",
    "softmax = nn.Softmax(dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = #insert file path here\n",
    "\n",
    "test_folder = #insert file path here\n",
    "\n",
    "val_folder = #insert file path here\n",
    "\n",
    "train_data = RooftopDataset(train_folder,transform = transform)\n",
    "\n",
    "test_data = RooftopDataset(test_folder,transform = transform)\n",
    "\n",
    "val_data = RooftopDataset(val_folder,transform = transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "train_losses , val_losses = [],[]\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "model = AlbedoEstimator(num_colors=2)\n",
    "model.to(device)\n",
    "\n",
    "#Establish loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#Get an optimizer, for now I can just use ADAM \n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = AlbedoEstimator(num_colors=2)\n",
    "model.to(device)\n",
    "\n",
    "# Establish loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get an optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc='Training loop'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        classifications = model(images)\n",
    "        loss = loss_function(classifications, labels)  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc='Validation loop'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "         \n",
    "            classifications = model(images)\n",
    "            loss = loss_function(classifications, labels)  # Use raw logits here\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "    val_loss = running_loss / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss}, Validation loss: {val_loss}\")\n",
    "\n",
    "# Get the probabilities for each classification. These will be interpreted as the albedo values\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images.to(device)\n",
    "        classifications = model(images)\n",
    "        albedo = torch.softmax(classifications, dim=1) \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
